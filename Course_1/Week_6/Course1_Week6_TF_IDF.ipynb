{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "In this task Hadoop Streaming is used to process Wikipedia articles dump (/data/wiki/en_articles_part).\n",
    "\n",
    "The purpose of this task is to calculate tf*idf for each pair (word, article) from the Wikipedia dump. Apply the stop words filter to speed up calculations. Term frequency (tf) is a function depending on a term (word) and a document (article):\n",
    "\n",
    "tf(term, doc_id) = Nt/N,\n",
    "\n",
    "where Nt - quantity of particular term in the document, N - the total number of terms in the document (without stop words)\n",
    "\n",
    "Inverse document frequency (idf) is a function depends on a term:\n",
    "\n",
    "idf(term) = 1/log(1 + Dt),\n",
    "\n",
    "where Dt - number of documents in the dataset with the particular term.\n",
    "\n",
    "You can find more information here: https://en.wikipedia.xn--org/wiki/Tfidf-q82h but use just the formulas mentioned above.\n",
    "\n",
    "Dataset location: /data/wiki/en_articles_part\n",
    "\n",
    "Stop words list is in ‘/datasets/stop_words_en.txt’ file.\n",
    "\n",
    "Format: article_id <tab> article_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper\n",
    "\n",
    "Idea is simple: create an array of words of each article. Going through the article, check do we have a word in array (using try-exception mechanism). If exists: increase its count in parallel array containing amount of counts. If it's a new word - add it. It's not the most optimal solution, because scanning through array require some extra time with the increasing of size of articles, but it allows us to calculate everything we need in 1 single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python2.7\n",
    "import sys\n",
    "import re\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8') # required to convert to unicode\n",
    "\n",
    "path = 'stop_words_en.txt'\n",
    "\n",
    "def read_stop_words(file_path):\n",
    "    return set(word.strip().lower() for word in open(file_path))\n",
    "\n",
    "stop_words = read_stop_words(path)\n",
    "\n",
    "words_in_article = []\n",
    "cnt_in_article = []\n",
    "id_in_list = 0\n",
    "total_cnt_in_article = 0\n",
    "word_cnt = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        article_id, text = unicode(line.strip()).split('\\t', 1)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "    words_in_article = []\n",
    "    cnt_in_article = []\n",
    "    id_in_list = 0\n",
    "    word_cnt = 0\n",
    "    total_cnt_in_article = 0\n",
    "    text = re.sub(\"^\\W+|\\W+$\", \"\", text, flags=re.UNICODE)\n",
    "    words = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in stop_words:\n",
    "            continue  \n",
    "        word_cnt = word_cnt + 1\n",
    "        try:\n",
    "            id_in_list = words_in_article.index(word.lower())\n",
    "            cnt_in_article[id_in_list] = cnt_in_article[id_in_list] + 1\n",
    "        except ValueError as e:\n",
    "            words_in_article.append(word.lower())\n",
    "            cnt_in_article.append(1)\n",
    "            total_cnt_in_article = total_cnt_in_article + 1\n",
    "    for i in range(0,len(words_in_article)):\n",
    "        print \"%s\\t%f\\t%d\\t%f\\t%d\" % (words_in_article[i], float(cnt_in_article[i]), word_cnt, float(cnt_in_article[i]) / float(word_cnt), int(article_id)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducer Script\n",
    "\n",
    "Idea is simple: we sort out all articles by their word (so that, for example, all \"labor\" articles going one by one) and run through them counting amount of all of them and saving the result into mid-list\n",
    "\n",
    "Once the word is over, we 1 more time run through saved list, calculate TF * IDF for each article separetely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python2.7\n",
    "import sys\n",
    "import math\n",
    "\n",
    "current_key = None\n",
    "word_sum = 0\n",
    "lines_list = []\n",
    "IDF = float(0.0)\n",
    "print \"%s\" % (\"Start!\")\n",
    "articles_list = set()\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        word, cnt, all_cnt, tdf, article_id = line.strip().split('\\t', 4)\n",
    "        tdf = float(tdf)\n",
    "        all_cnt = int(all_cnt)\n",
    "    except ValueError as e:\n",
    "        lines_list = []\n",
    "        continue\n",
    "    if current_key != word:\n",
    "        if current_key:\n",
    "            IDF = float(float(1.0) / float(math.log(1 + word_sum)))\n",
    "            for i in range(0, len(lines_list)):\n",
    "                w, c, all_, t, a = lines_list[i].strip().split('\\t', 4)\n",
    "                print \"%s\\t%f\\t%f\\t%f\\t%d\\t%d\" % (w, float(c), float(t), IDF, int(a), word_sum) \n",
    "            lines_list = []\n",
    "        word_sum = 0\n",
    "        articles_list.clear()\n",
    "        current_key = word\n",
    "    word_sum += 1\n",
    "    lines_list.append(line)\n",
    "    articles_list.add(article_id)\n",
    "\n",
    "if current_key:\n",
    "    print \"%s\\t%d\" % (current_key, word_sum)\n",
    "    lines_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile parser.py\n",
    "#!/usr/bin/env python2.7\n",
    "import sys\n",
    "\n",
    "result=0.0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        word, cnt, tf, idf, article_id, word_sum = line.strip().split('\\t', 6)\n",
    "        tf = float(tf)\n",
    "        idf = float(idf)\n",
    "        article_id = int(article_id)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "    if word == 'labor' and article_id == 12:\n",
    "        result = float(tf) * float(idf)\n",
    "\n",
    "print >> sys.stdout, \"%f\" % result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "OUT_DIR=\"Week6_task1_\"$(date +\"%s%6N\")\n",
    "NUM_REDUCERS=8\n",
    "LOGS=\"stderr_logs.txt\"\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR} > /dev/null\n",
    "\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapred.jab.name=\"Streaming wordCount\" \\\n",
    "    -D mapreduce.job.reduces=1 \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "    -D map.output.key.field.separator \\\n",
    "    -D mapreduce.partition.keycomparator.options=-k1 \\\n",
    "    -files mapper.py,reducer.py,/datasets/stop_words_en.txt \\\n",
    "    -mapper \"python mapper.py\" \\\n",
    "    -reducer \"python reducer.py\" \\\n",
    "    -input /data/wiki/en_articles_part \\\n",
    "    -output ${OUT_DIR}  > /dev/null\n",
    "    \n",
    "hdfs dfs -cat ${OUT_DIR}/part-* > result.txt\n",
    "cat result.txt | grep -w 'labor' | grep -w '12' | python2 parser.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
