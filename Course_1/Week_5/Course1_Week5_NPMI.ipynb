{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I am using only one summarized cell just because not sure how exactly Yandex & MPTI Engine works and how it will work will multiple cells **\n",
    "\n",
    "Ideally without any doubt it have to big series of multiple cells to track down the progress, improve readibility, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "sc = SparkContext(conf=SparkConf().setAppName(\"MyApp\").setMaster(\"local\"))\n",
    "\n",
    "import re\n",
    "import math\n",
    "\n",
    "# standard function which is parsing lines into words\n",
    "\n",
    "def parse_article(line):\n",
    "    try:\n",
    "        article_id, text = unicode(line.rstrip()).split('\\t', 1)\n",
    "        text = re.sub(\"^\\W+|\\W+$\", \"\", text, flags=re.UNICODE)\n",
    "        words = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n",
    "        return words\n",
    "    except ValueError as e:\n",
    "        return []\n",
    "\n",
    "# instead of stroing data by arcticles, I will create RDD with all words separate\n",
    "wiki_by_word = sc.textFile(\"/data/wiki/en_articles_part/articles-part\", 16).flatMap(parse_article) \\\n",
    ".map(lambda word: word.lower()).cache()\n",
    "\n",
    "# loading all stop words into other rdd\n",
    "stop_words = sc.textFile(\"/datasets/stop_words_en.txt\", 16).map(lambda word: '%s' % word.lower()).collect()\n",
    "\n",
    "# filtering all words by skipping stop words + indexing and reverting for future join\n",
    "filtered = wiki_by_word.filter(lambda word : word not in stop_words).zipWithIndex() \\\n",
    ".map(lambda (key, index) : (index, key)).cache()\n",
    "\n",
    "# creating variable to safe amount of all numbers\n",
    "num_of_words_total = int(filtered.count())\n",
    "\n",
    "# using list of filtered words, counting amount of their appeareance \n",
    "filtered_agg = filtered.map(lambda (key, index) : (index, 1)).reduceByKey(lambda a, b: a + b).cache()\n",
    "\n",
    "# increasing index for tricky join to get all pairs\n",
    "filtered_next_word = filtered.map(lambda (key,index): (key + 1, index)).cache()\n",
    "\n",
    "# joining to get separated pairs\n",
    "joined = filtered_next_word.join(filtered).cache()\n",
    "\n",
    "# number of pairs\n",
    "num_of_pairs = int(joined.count())\n",
    "\n",
    "# concatenat the result into pair\n",
    "joined_concat = joined.map(lambda (key, (val1, val2)): (val1 + '_' + val2, 1)).cache()\n",
    "\n",
    "# filtering the pairs to safe only 500 occurencies\n",
    "result = joined_concat.reduceByKey(lambda a, b: a + b).map(lambda (key, index) : (index, key)) \\\n",
    ".sortByKey(ascending=False).filter(lambda (key, index) : key > 499).cache()\n",
    "\n",
    "# Start to play games: let's safe pairs and splitted words together\n",
    "result_splitted = result.map(lambda (key, index) : (key, index, index.split(\"_\"))).cache()\n",
    "\n",
    "# a - count of pairs, b - pair itself, c[0] - 1st word, c[1] - 2nd word\n",
    "words_with_numbers=result_splitted.map(lambda (a, b, c) : (a, b, c[0], c[1])).cache()\n",
    "\n",
    "# this is really crazy transformaton when we join 2 times lost of words to get count of this word for future\n",
    "# and calc all probabilities at once\n",
    "big_rdd = words_with_numbers.map(lambda (a,b,c,d): (c,(a,b,d))).join(filtered_agg) \\\n",
    ".map(lambda (a,((b,c,d),e)) : (float(b)/float(num_of_pairs),c,float(e)/float(num_of_words_total),a,d)) \\\n",
    ".map(lambda (a,b,c,d,e): (e,(a,b,c,d))).join(filtered_agg) \\\n",
    ".map(lambda (a,((b,c,d,e), g)) : (b,c,d,e, float(g) / float(num_of_words_total), a)) \\\n",
    ".cache()\n",
    "\n",
    "# calc PMI\n",
    "big_rdd_with_pmi = big_rdd.map(lambda (a,b,c,d,e,f) : (math.log(a/(c*e)), a, b)).cache()\n",
    "\n",
    "# calc NPMI\n",
    "big_rdd_with_npmi = big_rdd_with_pmi.map(lambda (a,b,c): (a/((-1)*math.log(b)), c)) \\\n",
    ".sortByKey(ascending=False).cache()\n",
    "\n",
    "# amount of elements\n",
    "final_cnt = big_rdd_with_npmi.count()\n",
    "\n",
    "# print output\n",
    "for i in range(0, final_cnt):\n",
    "    print '%s' % big_rdd_with_npmi.collect()[i][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
